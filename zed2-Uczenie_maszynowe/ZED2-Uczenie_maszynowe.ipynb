{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''''\n",
    "Raport z uczenia maszynowego.\n",
    "\n",
    "1. Wczytanie bibliotek wykorzystywanych w projekcie\n",
    "2. Wczytanie danych, usunięcie wartości niedozwolonych, usunięcie duplikatów res_name i pdb_code oraz\n",
    "pozostawienie klas o liczności >=5 \n",
    "3. Wstępne przetwarzanie danych: pozostawienie kolumn numerycznych, usunięcie kolumn zawierających same NaN oraz\n",
    "kolumn wypełnionych jedną wartością.\n",
    "4. Wczytywanie danych testowych i poddanie i przetwarzaniu analogicznemu do punktu 3. Usunięcie ze zbioru uczącego niedostępnych\n",
    "w zbiorze testowym kolumn. Kolumna 'local_res_atom_O_count' nie została usunięta, ponieważ wtedy klasyfikator przestawał \n",
    "działać. Nie potrafiłam naprawić tego problemu, więc dodałam do zbioru uczącego kolumnę wypełnioną zerami, zapewne pogorszy to \n",
    "nieco wynik klasyfikacji. Wypełnienie wartości NaN wartościami średnimi kolumn.\n",
    "5. Tworzenie klasyfikatora z etykiet res_name. Aby skorzystać z funkcji recall_score() należało odpowiednio przetransformować \n",
    "etykiety. Następnie korzystałam z RandomForestClassifier() oraz funkcji RandomizedSearchCV() w celu zoptymalizowania parametrów.\n",
    "Dla zbioru danych, który zawierał kolumny brakujące w zbiorze testowym, recall utrzymywał się na poziomie powyżej 0.82. Po \n",
    "usunięciu tych kolumn miara spadła do 0.47.\n",
    "6. Tworzenie klasyfikatora z etykiet res_name_grouped. Dla zbioru danych, który zawierał kolumny brakujące w zbiorze testowym, \n",
    "recall utrzymywał się na poziomie powyżej 0.89. Pousunięciu tych kolumn miara spadła do 0.51.\n",
    "7. Predykcja wartości dla zbioru testowego i zapis do pliku.\n",
    "8. Serializacja za pomocą biblioteki joblib. Parametry zostały podane ręcznie dla najlepszych uzyskanych wyników podczas testów.\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Projekt powinien być napisany w języku Python z wykorzystaniem bibliotek Pandas i scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mae\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\parsers.py:1159: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "#wczytywanie danych z pliku\n",
    "df = pd.read_csv(\"all_summary.csv\", sep=\";\", na_values=[\"n/a\"])\n",
    "'''\n",
    "Ze zbioru danych powinny zostać usunięte wiersze posiadające wartość zmiennej \n",
    "res_name równą: “DA”,“DC”,“DT”, “DU”, “DG”, “DI”,“UNK”, “UNX”, “UNL”, “PR”, “PD”, “Y1”, “EU”, “N”, “15P”, “UQ”, “PX4” lub “NAN”;\n",
    "'''\n",
    "forbidden_values = [\"DA\",\"DC\",\"DT\", \"DU\", \"DG\", \"DI\",\"UNK\", \"UNX\", \"UNL\", \"PR\", \"PD\", \"Y1\", \"EU\", \"N\", \"15P\", \"UQ\", \"PX4\", \"NAN\"]\n",
    "df = df[~df.res_name.isin(forbidden_values)]\n",
    "\n",
    "#W zbiorze danych uczących powinny zostać tylko unikatowe pary (pdb_code, res_name)\n",
    "df = df.drop_duplicates(['res_name', 'pdb_code'])\n",
    "\n",
    "#Klasyfikator uczony na oryginalnym zbiorze etykiet (res_name) powinien korzystać ze wszystkich klas,\n",
    "#których liczność wynosi co najmniej 5 przykładów; klasy o mniejszej liczności nie mają być brane pod uwagę\n",
    "value_counts = df['res_name'].value_counts()\n",
    "to_remove = value_counts[value_counts<5].index\n",
    "liczba = value_counts[value_counts<5]\n",
    "df = df[~df.res_name.isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wstępne przetwarzanie\n",
    "\n",
    "X = df.drop('res_name', axis=1)\n",
    "#zostawia tylko kolumny numeryczne\n",
    "X = X._get_numeric_data()\n",
    "#pozbywa się kolumn wypełnonych NaN\n",
    "X = X.drop([\"local_BAa\", \"local_NPa\", \"local_Ra\", \"local_RGa\", \"local_SRGa\", \"local_CCPa\", \"local_ZOa\", \"local_ZDa\", \"local_ZD_minus_a\", \"local_CCSa\", \"local_ZD_plus_a\", \"weight_col\"], axis=1)\n",
    "#pozbywa się kolumn wypełnionych w całości tymi samymi wartościami\n",
    "X = X.drop([\"local_min\", \"grid_space\", \"solvent_radius\", \"solvent_opening_radius\", \"resolution_max_limit\", \"part_step_FoFc_std_min\", \"part_step_FoFc_std_max\", \"part_step_FoFc_std_step\" ], axis=1)\n",
    "#print(X.columns.get_loc(\"part_00_blob_electron_sum\"))\n",
    "#print(X.columns.get_loc(\"part_09_density_sqrt_E3\"))\n",
    "#usuwanie kolumn zaczynających się od part_0X, z uwagi na dużo wartości NaN i wzajemną korelację\n",
    "X = X.drop(X.columns[15:705], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wczytywanie danych do predykcji wartości\n",
    "td = pd.read_csv(\"test_data.txt\", sep=\",\", na_values=[\"n/a\"])\n",
    "\n",
    "#print(test_data.columns.get_loc(\"part_00_blob_electron_sum\"))\n",
    "#print(test_data.columns.get_loc(\"part_10_density_sqrt_E3\"))\n",
    "\n",
    "#oczyszczanie kolumn z partXX\n",
    "X_td = td.drop(td.columns[1:760], axis=1)\n",
    "\n",
    "#zostawia tylko kolumny numeryczne\n",
    "X_td = X_td._get_numeric_data()\n",
    "#pozbywa się kolumn wypełnionych w całości tymi samymi wartościami i id\n",
    "X_td = X_td.drop([\"local_min\", \"Unnamed: 0\"], axis=1)\n",
    "\n",
    "#tworzenie list niedostępnych kolumn\n",
    "unavailable_columns=[]\n",
    "for col in X.columns.values:\n",
    "    if col not in X_td.columns.values:\n",
    "        unavailable_columns.append(col)\n",
    "        \n",
    "    \n",
    "#usuwanie ze zbioru uczącego niedostępych kolumn\n",
    "X = X.drop(['local_res_atom_count','local_res_atom_non_h_count', 'local_res_atom_non_h_occupancy_sum', 'local_res_atom_non_h_electron_sum', 'local_res_atom_non_h_electron_occupancy_sum', 'local_res_atom_C_count', 'local_res_atom_N_count', 'local_res_atom_S_count', 'dict_atom_non_h_count', 'dict_atom_non_h_electron_sum', 'dict_atom_C_count', 'dict_atom_N_count', 'dict_atom_O_count', 'dict_atom_S_count'], axis=1)\n",
    "\n",
    "'''\n",
    "Kolumna 'local_res_atom_O_count' nie została usunięta, ponieważ wtedy przestawał działać random_forest, \n",
    "nie potraiłam sobie poradzić z tym błędem. Dlatego zdecydowałam się na dodawnie kolumny wypełnionej zerami\n",
    "do testowego data frame'a. Może to negatywnie wpłynąć na wyniki klasyfikacji.\n",
    "\n",
    "'''\n",
    "column = pd.DataFrame()\n",
    "column['local_res_atom_O_count'] = pd.Series(0, index=X_td.index)\n",
    "X_td = pd.concat([column, X_td], axis=1)\n",
    "\n",
    "#wypełnianie wartości NaN wartościami średnimi (maksymalnie stanowią ok. 1% wartości kolumn wynosi NaN)\n",
    "X = X.fillna(X.mean())\n",
    "y = df[\"res_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.16864, std: 0.01177, params: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 10, 'criterion': 'gini', 'max_features': 2, 'max_depth': 3}, mean: 0.42282, std: 0.01711, params: {'bootstrap': True, 'min_samples_leaf': 2, 'min_samples_split': 4, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}, mean: 0.42970, std: 0.02214, params: {'bootstrap': False, 'min_samples_leaf': 8, 'min_samples_split': 7, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}, mean: 0.33688, std: 0.01361, params: {'bootstrap': False, 'min_samples_leaf': 8, 'min_samples_split': 8, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}, mean: 0.25964, std: 0.00972, params: {'bootstrap': True, 'min_samples_leaf': 5, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 8, 'max_depth': 3}, mean: 0.24809, std: 0.01513, params: {'bootstrap': False, 'min_samples_leaf': 5, 'min_samples_split': 4, 'criterion': 'gini', 'max_features': 5, 'max_depth': 3}, mean: 0.30624, std: 0.00617, params: {'bootstrap': True, 'min_samples_leaf': 9, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': 3, 'max_depth': None}, mean: 0.36479, std: 0.01564, params: {'bootstrap': False, 'min_samples_leaf': 8, 'min_samples_split': 9, 'criterion': 'gini', 'max_features': 4, 'max_depth': None}, mean: 0.36765, std: 0.01284, params: {'bootstrap': False, 'min_samples_leaf': 6, 'min_samples_split': 1, 'criterion': 'entropy', 'max_features': 5, 'max_depth': None}, mean: 0.26379, std: 0.02606, params: {'bootstrap': True, 'min_samples_leaf': 7, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': 7, 'max_depth': 3}, mean: 0.21031, std: 0.02212, params: {'bootstrap': True, 'min_samples_leaf': 7, 'min_samples_split': 8, 'criterion': 'entropy', 'max_features': 3, 'max_depth': 3}, mean: 0.22004, std: 0.02786, params: {'bootstrap': True, 'min_samples_leaf': 5, 'min_samples_split': 4, 'criterion': 'gini', 'max_features': 4, 'max_depth': 3}, mean: 0.40997, std: 0.02362, params: {'bootstrap': False, 'min_samples_leaf': 10, 'min_samples_split': 8, 'criterion': 'entropy', 'max_features': 8, 'max_depth': None}, mean: 0.24419, std: 0.02585, params: {'bootstrap': True, 'min_samples_leaf': 4, 'min_samples_split': 7, 'criterion': 'gini', 'max_features': 4, 'max_depth': 3}, mean: 0.18512, std: 0.00368, params: {'bootstrap': False, 'min_samples_leaf': 6, 'min_samples_split': 1, 'criterion': 'entropy', 'max_features': 2, 'max_depth': 3}, mean: 0.31598, std: 0.01695, params: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 9, 'criterion': 'entropy', 'max_features': 3, 'max_depth': None}, mean: 0.29015, std: 0.01829, params: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 2, 'max_depth': None}, mean: 0.41789, std: 0.01315, params: {'bootstrap': True, 'min_samples_leaf': 5, 'min_samples_split': 9, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}, mean: 0.41296, std: 0.01853, params: {'bootstrap': True, 'min_samples_leaf': 8, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}, mean: 0.26133, std: 0.01070, params: {'bootstrap': True, 'min_samples_leaf': 2, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': 6, 'max_depth': 3}]\n",
      "0.470926711084\n"
     ]
    }
   ],
   "source": [
    "#klasyfikator na etykietach res_name\n",
    "\n",
    "#transformacja etykiet, konieczna do skorzystania z funkcji recall_score()\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y.values)\n",
    "\n",
    "#podział na zbiór testowy i treninogwy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# budowanie klasyfikatora random forest\n",
    "clf1 = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "# specifikowanie parametrów do randomized search\n",
    "param_dist1 = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# odpalenie randomized search\n",
    "n_iter_search1 = 20\n",
    "random_search1 = RandomizedSearchCV(clf1, param_distributions=param_dist1, n_iter=n_iter_search1)\n",
    "\n",
    "#uzyskanie najlepszych parametrów\n",
    "forest = random_search1.fit(X_train, y_train)\n",
    "#print(random_search1.grid_scores_)\n",
    "\n",
    "#predykcja dla zbioru testowego\n",
    "y_pred=forest.predict(X_test)\n",
    "y_true=y_test.values\n",
    "y_true_norm=le.transform(y_true)\n",
    "y_pred_norm=le.transform(y_pred)\n",
    "\n",
    "#estymacja miary recall\n",
    "print(recall_score(y_true_norm, y_pred_norm, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.41568, std: 0.02349, params: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'entropy', 'max_features': 5, 'max_depth': None}, mean: 0.28690, std: 0.01117, params: {'bootstrap': True, 'min_samples_leaf': 2, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 7, 'max_depth': 3}, mean: 0.25094, std: 0.00689, params: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 2, 'criterion': 'gini', 'max_features': 4, 'max_depth': 3}, mean: 0.49137, std: 0.02595, params: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 6, 'criterion': 'entropy', 'max_features': 10, 'max_depth': None}, mean: 0.29170, std: 0.01879, params: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 9, 'criterion': 'gini', 'max_features': 7, 'max_depth': 3}, mean: 0.48332, std: 0.03682, params: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'entropy', 'max_features': 9, 'max_depth': None}, mean: 0.48254, std: 0.01645, params: {'bootstrap': False, 'min_samples_leaf': 9, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 9, 'max_depth': None}, mean: 0.44918, std: 0.02057, params: {'bootstrap': True, 'min_samples_leaf': 5, 'min_samples_split': 6, 'criterion': 'gini', 'max_features': 6, 'max_depth': None}, mean: 0.38427, std: 0.02389, params: {'bootstrap': True, 'min_samples_leaf': 10, 'min_samples_split': 6, 'criterion': 'entropy', 'max_features': 4, 'max_depth': None}, mean: 0.27249, std: 0.01126, params: {'bootstrap': False, 'min_samples_leaf': 8, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 6, 'max_depth': 3}, mean: 0.29975, std: 0.02291, params: {'bootstrap': False, 'min_samples_leaf': 3, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 8, 'max_depth': 3}, mean: 0.32065, std: 0.02781, params: {'bootstrap': True, 'min_samples_leaf': 7, 'min_samples_split': 8, 'criterion': 'entropy', 'max_features': 9, 'max_depth': 3}, mean: 0.30053, std: 0.02722, params: {'bootstrap': False, 'min_samples_leaf': 4, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 7, 'max_depth': 3}, mean: 0.24575, std: 0.02334, params: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 7, 'criterion': 'gini', 'max_features': 4, 'max_depth': 3}, mean: 0.32039, std: 0.00994, params: {'bootstrap': False, 'min_samples_leaf': 7, 'min_samples_split': 9, 'criterion': 'entropy', 'max_features': 2, 'max_depth': None}, mean: 0.33545, std: 0.02563, params: {'bootstrap': True, 'min_samples_leaf': 6, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': 9, 'max_depth': 3}, mean: 0.29962, std: 0.01394, params: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 2, 'criterion': 'gini', 'max_features': 10, 'max_depth': 3}, mean: 0.32312, std: 0.01687, params: {'bootstrap': True, 'min_samples_leaf': 10, 'min_samples_split': 8, 'criterion': 'gini', 'max_features': 10, 'max_depth': 3}, mean: 0.30650, std: 0.02127, params: {'bootstrap': True, 'min_samples_leaf': 7, 'min_samples_split': 6, 'criterion': 'entropy', 'max_features': 2, 'max_depth': None}, mean: 0.26470, std: 0.01342, params: {'bootstrap': False, 'min_samples_leaf': 10, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 5, 'max_depth': 3}]\n",
      "0.516656571775\n"
     ]
    }
   ],
   "source": [
    "#klasyfikator na etykietach zgrupowanych\n",
    "\n",
    "#wczytanie etykiet zgrupowanych\n",
    "df_grouped = pd.read_csv(\"grouped_res_name.txt\", sep=\",\", na_values=[\"n/a\"])\n",
    "y_grouped = df_grouped[\"res_name_group\"]\n",
    "\n",
    "#normalizacja etykiet zgrupowanych\n",
    "le_grouped = preprocessing.LabelEncoder()\n",
    "le_grouped.fit(y_grouped.values)\n",
    "\n",
    "#podział na zbiór testowy i treninogwy dla etykiet zgrupowanych\n",
    "X_train_grouped, X_test_grouped, y_train_grouped, y_test_grouped = train_test_split(X, y_grouped, test_size=0.3, random_state=0)\n",
    "\n",
    "# budowanie klasyfikatora random forest\n",
    "clf_grouped = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "# specifikowanie parametrów do randomized search\n",
    "param_dist_grouped = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# odpalenie randomized search\n",
    "n_iter_search_grouped = 20\n",
    "random_search_grouped = RandomizedSearchCV(clf_grouped, param_distributions=param_dist_grouped, n_iter=n_iter_search_grouped)\n",
    "\n",
    "#uzyskanie najlepszych parametrów dla etykiet zgrupowanych\n",
    "forest_grouped = random_search_grouped.fit(X_train_grouped, y_train_grouped)\n",
    "print(random_search_grouped.grid_scores_)\n",
    "\n",
    "#predykcja dla zbioru testowego etykiet zgrupowanych\n",
    "y_pred_grouped=forest_grouped.predict(X_test_grouped)\n",
    "y_true_grouped = y_test_grouped.values\n",
    "y_true_grouped_norm=le_grouped.transform(y_true_grouped)\n",
    "y_pred_grouped_norm=le_grouped.transform(y_pred_grouped)\n",
    "\n",
    "#estymacja miary recall\n",
    "print(recall_score(y_true_grouped_norm, y_pred_grouped_norm, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predykcja wyników dla test_data.txt\n",
    "\n",
    "X_td = X_td.fillna(X_td.mean())\n",
    "\n",
    "y_test_pred=forest.predict(X_td)\n",
    "\n",
    "file_ = open('predykcja1.txt','w')\n",
    "j=0\n",
    "file_.write('\"\",\"res_name\"\\n')\n",
    "for i in y_test_pred:\n",
    "    file_.write('\"{}\",\"{}\"\\n'.format(j,i))\n",
    "    j += 1\n",
    "file_.close()\n",
    "\n",
    "\n",
    "y_test_pred_grouped=forest_grouped.predict(X_td)\n",
    "\n",
    "file_ = open('predykcja2.txt','w')\n",
    "j=0\n",
    "file_.write('\"\",\"res_name_group\"\\n')\n",
    "for i in y_test_pred_grouped:\n",
    "    file_.write('\"{}\",\"{}\"\\n'.format(j,i))\n",
    "    j += 1\n",
    "file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf2.pkl',\n",
       " 'clf2.pkl_01.npy',\n",
       " 'clf2.pkl_02.npy',\n",
       " 'clf2.pkl_03.npy',\n",
       " 'clf2.pkl_04.npy',\n",
       " 'clf2.pkl_05.npy',\n",
       " 'clf2.pkl_06.npy',\n",
       " 'clf2.pkl_07.npy',\n",
       " 'clf2.pkl_08.npy',\n",
       " 'clf2.pkl_09.npy',\n",
       " 'clf2.pkl_10.npy',\n",
       " 'clf2.pkl_11.npy',\n",
       " 'clf2.pkl_12.npy',\n",
       " 'clf2.pkl_13.npy',\n",
       " 'clf2.pkl_14.npy',\n",
       " 'clf2.pkl_15.npy',\n",
       " 'clf2.pkl_16.npy',\n",
       " 'clf2.pkl_17.npy',\n",
       " 'clf2.pkl_18.npy',\n",
       " 'clf2.pkl_19.npy',\n",
       " 'clf2.pkl_20.npy',\n",
       " 'clf2.pkl_21.npy',\n",
       " 'clf2.pkl_22.npy',\n",
       " 'clf2.pkl_23.npy',\n",
       " 'clf2.pkl_24.npy',\n",
       " 'clf2.pkl_25.npy',\n",
       " 'clf2.pkl_26.npy',\n",
       " 'clf2.pkl_27.npy',\n",
       " 'clf2.pkl_28.npy',\n",
       " 'clf2.pkl_29.npy',\n",
       " 'clf2.pkl_30.npy',\n",
       " 'clf2.pkl_31.npy',\n",
       " 'clf2.pkl_32.npy',\n",
       " 'clf2.pkl_33.npy',\n",
       " 'clf2.pkl_34.npy',\n",
       " 'clf2.pkl_35.npy',\n",
       " 'clf2.pkl_36.npy',\n",
       " 'clf2.pkl_37.npy',\n",
       " 'clf2.pkl_38.npy',\n",
       " 'clf2.pkl_39.npy',\n",
       " 'clf2.pkl_40.npy',\n",
       " 'clf2.pkl_41.npy',\n",
       " 'clf2.pkl_42.npy',\n",
       " 'clf2.pkl_43.npy',\n",
       " 'clf2.pkl_44.npy',\n",
       " 'clf2.pkl_45.npy',\n",
       " 'clf2.pkl_46.npy',\n",
       " 'clf2.pkl_47.npy',\n",
       " 'clf2.pkl_48.npy',\n",
       " 'clf2.pkl_49.npy',\n",
       " 'clf2.pkl_50.npy',\n",
       " 'clf2.pkl_51.npy',\n",
       " 'clf2.pkl_52.npy',\n",
       " 'clf2.pkl_53.npy',\n",
       " 'clf2.pkl_54.npy',\n",
       " 'clf2.pkl_55.npy',\n",
       " 'clf2.pkl_56.npy',\n",
       " 'clf2.pkl_57.npy',\n",
       " 'clf2.pkl_58.npy',\n",
       " 'clf2.pkl_59.npy',\n",
       " 'clf2.pkl_60.npy',\n",
       " 'clf2.pkl_61.npy',\n",
       " 'clf2.pkl_62.npy',\n",
       " 'clf2.pkl_63.npy',\n",
       " 'clf2.pkl_64.npy',\n",
       " 'clf2.pkl_65.npy',\n",
       " 'clf2.pkl_66.npy',\n",
       " 'clf2.pkl_67.npy',\n",
       " 'clf2.pkl_68.npy',\n",
       " 'clf2.pkl_69.npy',\n",
       " 'clf2.pkl_70.npy',\n",
       " 'clf2.pkl_71.npy',\n",
       " 'clf2.pkl_72.npy',\n",
       " 'clf2.pkl_73.npy',\n",
       " 'clf2.pkl_74.npy',\n",
       " 'clf2.pkl_75.npy',\n",
       " 'clf2.pkl_76.npy',\n",
       " 'clf2.pkl_77.npy',\n",
       " 'clf2.pkl_78.npy',\n",
       " 'clf2.pkl_79.npy',\n",
       " 'clf2.pkl_80.npy',\n",
       " 'clf2.pkl_81.npy']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Randomized search za każdym razem zwraca inne wyniki, zatem utworzyłam ręcznie klasyfikatory do serializacji z parametrów,\n",
    "które dawały najlepsze wyniki.\n",
    "Dla res_name recall wynosi: 0.470926711084\n",
    "Dla res_name_grouped recall wynosi: 0.516656571775\n",
    "'''\n",
    "\n",
    "clf1_bestparams = RandomForestClassifier(n_estimators=20, bootstrap=True, min_samples_leaf = 2, min_samples_split=4, criterion='gini', max_features=10, max_depth= None)\n",
    "clf_grouped_bestparams = RandomForestClassifier(n_estimators=20, bootstrap=False, min_samples_leaf = 7, min_samples_split=6, criterion='entropy', max_features=10, max_depth= None)\n",
    "clf1_bestparams.fit(X_train, y_train)\n",
    "clf_grouped_bestparams.fit(X_train_grouped, y_train_grouped)\n",
    "    \n",
    "joblib.dump(clf1_bestparams, 'clf1.pkl')\n",
    "joblib.dump(clf_grouped_bestparams, 'clf2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
